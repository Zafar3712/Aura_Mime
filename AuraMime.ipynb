{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d360355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Loading models (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if __name__ == \"__main__\":\\n    # 0) Record from mic (press \\'s\\' to start, \\'q\\' to stop)\\n    try:\\n        # Only record if neutral_input.wav doesn\\'t already exist or you want to re-record\\n        if not os.path.exists(FILENAME):\\n            print(\"No existing recording found. Starting recorder.\")\\n            start_recording()\\n        else:\\n            print(f\"Using existing file {FILENAME} (delete it to re-record).\")\\n\\n        # 1) Process file\\n        process_audio_file(FILENAME)\\n\\n    except Exception as main_e:\\n        print(\"Fatal error:\", main_e)'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fixed and consolidated audio -> split -> audio-emotion + text-emotion pipeline.\n",
    "\n",
    "Key fixes:\n",
    "- Correct, robust recording loop using sounddevice + pynput.\n",
    "- Save chunks to \"chunks/\" folder and check paths before using them.\n",
    "- Use whisper.transcribe(file_path) instead of passing raw numpy array.\n",
    "- Replace deprecated return_all_scores with top_k=None in transformers pipeline.\n",
    "- Robust handling if silence splitting creates zero chunks (falls back to whole file).\n",
    "- Clean imports and helpful debug prints.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from pydub import AudioSegment, silence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline, AutoModelForAudioClassification, AutoFeatureExtractor\n",
    "import whisper\n",
    "\n",
    "# Optional: speechbrain import if you need it elsewhere (kept from your original file)\n",
    "# from speechbrain.inference import EncoderClassifier\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "FILENAME = \"neutral_input.wav\"\n",
    "SAMPLERATE = 16000\n",
    "CHUNKS_DIR = \"chunks\"\n",
    "MIN_SILENCE_LEN = 500     # ms\n",
    "SILENCE_THRESH_DELTA = 10 # dB below audio dBFS\n",
    "KEEP_SILENCE_MS = 150     # ms to keep at edges\n",
    "\n",
    "# ---------------- Recording utilities ----------------\n",
    "recording_started = threading.Event()\n",
    "recording_stopped = threading.Event()\n",
    "_recorded_frames = []  # store incoming frames (per-callback)\n",
    "\n",
    "def _audio_callback(indata, frames, time, status):\n",
    "    # indata is shape (frames, channels)\n",
    "    if recording_started.is_set() and not recording_stopped.is_set():\n",
    "        # copy to avoid referencing the same buffer\n",
    "        _recorded_frames.append(indata.copy())\n",
    "\n",
    "def start_recording(filename=FILENAME, samplerate=SAMPLERATE):\n",
    "    \"\"\"\n",
    "    Start recording using sounddevice. Press 's' to start and 'q' to stop (via pynput keyboard).\n",
    "    Saves WAV to `filename`.\n",
    "    \"\"\"\n",
    "    from pynput import keyboard\n",
    "\n",
    "    # reset state\n",
    "    _recorded_frames.clear()\n",
    "    recording_started.clear()\n",
    "    recording_stopped.clear()\n",
    "\n",
    "    print(\"🎙️ Press 's' to start recording, and 'q' to stop.\")\n",
    "\n",
    "    def on_press(key):\n",
    "        try:\n",
    "            if key.char == 's' and not recording_started.is_set():\n",
    "                print(\"🔴 Recording started...\")\n",
    "                recording_started.set()\n",
    "            elif key.char == 'q' and recording_started.is_set():\n",
    "                print(\"🛑 Recording stopped.\")\n",
    "                recording_stopped.set()\n",
    "                # don't return False here because we are using listener.stop() below\n",
    "        except AttributeError:\n",
    "            # special keys ignored\n",
    "            pass\n",
    "\n",
    "    listener = keyboard.Listener(on_press=on_press)\n",
    "    listener.start()\n",
    "\n",
    "    # Start audio stream and wait until stopped\n",
    "    try:\n",
    "        with sd.InputStream(callback=_audio_callback, samplerate=samplerate, channels=1):\n",
    "            # wait loop (non-busy)\n",
    "            while not recording_stopped.is_set():\n",
    "                sd.sleep(100)\n",
    "    finally:\n",
    "        listener.stop()\n",
    "\n",
    "    if len(_recorded_frames) == 0:\n",
    "        raise RuntimeError(\"No audio recorded. Did you press 's' to start?\")\n",
    "\n",
    "    # Concatenate frames, flatten channel dimension\n",
    "    audio_np = np.concatenate(_recorded_frames, axis=0)\n",
    "    # audio_np shape: (n_samples, 1) -> flatten to (n_samples,)\n",
    "    if audio_np.ndim > 1:\n",
    "        audio_np = audio_np.reshape(-1)\n",
    "\n",
    "    # soundfile expects float32 or int16 etc.\n",
    "    sf.write(filename, audio_np, samplerate, subtype='PCM_16')\n",
    "    print(f\"✅ Audio saved to {filename} (samples: {audio_np.shape[0]}, sr: {samplerate})\")\n",
    "    return filename\n",
    "\n",
    "# ---------------- pydub helpers ----------------\n",
    "def float_to_audiosegment(audio: np.ndarray, sr: int):\n",
    "    \"\"\"\n",
    "    Convert float numpy array in [-1,1] to pydub.AudioSegment (16-bit PCM mono).\n",
    "    \"\"\"\n",
    "    # ensure float32\n",
    "    audio_f = audio.astype(np.float32)\n",
    "    # clip\n",
    "    audio_f = np.clip(audio_f, -1.0, 1.0)\n",
    "    # convert to int16\n",
    "    audio_int16 = (audio_f * 32767).astype(np.int16)\n",
    "    return AudioSegment(\n",
    "        audio_int16.tobytes(),\n",
    "        frame_rate=sr,\n",
    "        sample_width=2,\n",
    "        channels=1\n",
    "    )\n",
    "\n",
    "def split_audio_on_silence_from_file(filepath, min_silence_len=MIN_SILENCE_LEN, silence_thresh_delta=SILENCE_THRESH_DELTA, keep_silence=KEEP_SILENCE_MS):\n",
    "    \"\"\"\n",
    "    Load WAV file, convert to pydub segment and split on silence.\n",
    "    Returns list of AudioSegment chunks (may be empty).\n",
    "    \"\"\"\n",
    "    segment = AudioSegment.from_file(filepath, format=\"wav\")\n",
    "    # dynamic threshold relative to file loudness:\n",
    "    silence_thresh = segment.dBFS - silence_thresh_delta\n",
    "    chunks = silence.split_on_silence(\n",
    "        segment,\n",
    "        min_silence_len=min_silence_len,\n",
    "        silence_thresh=silence_thresh,\n",
    "        keep_silence=keep_silence\n",
    "    )\n",
    "    print(f\"🔪 Split into {len(chunks)} chunks (silence_thresh={silence_thresh:.1f} dBFS)\")\n",
    "    return chunks\n",
    "\n",
    "def save_chunks(chunks, out_dir=CHUNKS_DIR, prefix=\"chunk\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    paths = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        filename = os.path.join(out_dir, f\"{prefix}_{i}.wav\")\n",
    "        chunk.export(filename, format=\"wav\")\n",
    "        paths.append(filename)\n",
    "    return paths\n",
    "\n",
    "# ---------------- Visualization (optional) ----------------\n",
    "def plot_waveform(filepath, sr=SAMPLERATE, figsize=(14,4), ytick_step=0.1, save_path=None):\n",
    "    audio, _ = librosa.load(filepath, sr=sr, mono=True)\n",
    "    duration = len(audio) / sr\n",
    "    time = np.linspace(0., duration, len(audio))\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(time, audio)\n",
    "    plt.title(f\"Waveform: {os.path.basename(filepath)}\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.yticks(np.arange(-1.0, 1.1, ytick_step))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # New save logic\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"📊 Waveform plot saved to {save_path}\")\n",
    "        plt.close() # Frees up memory\n",
    "    else:\n",
    "        try:\n",
    "            plt.show() # Keep original behavior as a fallback\n",
    "        except UserWarning:\n",
    "            pass # Ignore the \"non-interactive\" warning\n",
    "\n",
    "# ---------------- Model / Inference Setup ----------------\n",
    "print(\"🔁 Loading models (this may take a while)...\")\n",
    "# Audio emotion classifier (Hugging Face)\n",
    "AUDIO_MODEL_ID = \"firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3\"\n",
    "audio_model = AutoModelForAudioClassification.from_pretrained(AUDIO_MODEL_ID)\n",
    "audio_feature_extractor = AutoFeatureExtractor.from_pretrained(AUDIO_MODEL_ID)\n",
    "audio_id2label = audio_model.config.id2label\n",
    "\n",
    "def preprocess_audio_for_model(audio_path, feature_extractor, max_duration=30.0):\n",
    "    # librosa loads float32 in [-1,1]\n",
    "    audio_array, _ = librosa.load(audio_path, sr=feature_extractor.sampling_rate, mono=True)\n",
    "    max_len = int(feature_extractor.sampling_rate * max_duration)\n",
    "    if len(audio_array) > max_len:\n",
    "        audio_array = audio_array[:max_len]\n",
    "    else:\n",
    "        audio_array = np.pad(audio_array, (0, max_len - len(audio_array)))\n",
    "    inputs = feature_extractor(audio_array, sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "def predict_audio_emotion(audio_path, model, feature_extractor, id2label):\n",
    "    inputs = preprocess_audio_for_model(audio_path, feature_extractor)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = F.softmax(outputs.logits, dim=-1)\n",
    "    confidence, pred_id = torch.max(probs, dim=-1)\n",
    "    label = id2label[pred_id.item()]\n",
    "    return label, float(confidence.item())\n",
    "\n",
    "# Whisper (for transcription)\n",
    "whisper_model = whisper.load_model(\"base\")  # uses CPU if no GPU\n",
    "\n",
    "# Text emotion pipeline: use top_k=None to return all scores (replacement for return_all_scores)\n",
    "text_emotion_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    top_k=None,   # gives all scores\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "def predict_text_emotion(text, pipeline_obj=text_emotion_pipeline):\n",
    "    outputs = pipeline_obj(text)[0]  # returns list of dicts\n",
    "    # ensure sorted by score\n",
    "    outputs_sorted = sorted(outputs, key=lambda x: x['score'], reverse=True)\n",
    "    top = outputs_sorted[0]\n",
    "    return top['label'], float(top['score'])\n",
    "\n",
    "# ---------------- Main flow ----------------\n",
    "def process_audio_file(audio_file=FILENAME):\n",
    "    # 1) Optionally visualize\n",
    "    try:\n",
    "        plot_waveform(audio_file, sr=SAMPLERATE, figsize=(12,3), ytick_step=0.2, save_path=\"input_waveform.png\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) Split into chunks on silence\n",
    "    chunks = split_audio_on_silence_from_file(audio_file)\n",
    "    if len(chunks) == 0:\n",
    "        # fallback: use entire file as single chunk\n",
    "        print(\"⚠️ No silence-based chunks found — using the entire audio as one chunk.\")\n",
    "        full = AudioSegment.from_file(audio_file, format=\"wav\")\n",
    "        chunks = [full]\n",
    "\n",
    "    # save chunks\n",
    "    chunk_paths = save_chunks(chunks)\n",
    "    print(f\"✅ Saved {len(chunk_paths)} chunk files to '{CHUNKS_DIR}/'\")\n",
    "\n",
    "    # 3) For each chunk: audio-emotion, whisper transcription, text-emotion\n",
    "    audio_emotions_list = []\n",
    "    text_emotions_list = []\n",
    "    transcripts_list = []\n",
    "\n",
    "    print(\"\\n🎧 Combined Emotion Predictions (Audio + Text):\")\n",
    "    for path in chunk_paths:\n",
    "        try:\n",
    "            # audio-based emotion\n",
    "            audio_emotion, audio_conf = predict_audio_emotion(path, audio_model, audio_feature_extractor, audio_id2label)\n",
    "\n",
    "            # whisper transcription (use file path)\n",
    "            trans = whisper_model.transcribe(path, fp16=False)  # ensure fp16 off on CPU\n",
    "            transcript = trans.get(\"text\", \"\").strip()\n",
    "\n",
    "            # text-based emotion\n",
    "            if transcript:\n",
    "                text_emotion, text_conf = predict_text_emotion(transcript)\n",
    "                print(f\"{os.path.basename(path)} ➤ Audio: {audio_emotion} ({audio_conf:.2f}) | Text: {text_emotion} ({text_conf:.2f})\")\n",
    "                print(f\"📝 Transcript: \\\"{transcript}\\\"\\n\")\n",
    "            else:\n",
    "                text_emotion, text_conf = None, None\n",
    "                print(f\"{os.path.basename(path)} ➤ Audio: {audio_emotion} ({audio_conf:.2f}) | Text: [No speech detected]\\n\")\n",
    "\n",
    "            audio_emotions_list.append(audio_emotion)\n",
    "            text_emotions_list.append(text_emotion)\n",
    "            transcripts_list.append(transcript)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{os.path.basename(path)} ➤ Error: {e}\\n\")\n",
    "            audio_emotions_list.append(None)\n",
    "            text_emotions_list.append(None)\n",
    "            transcripts_list.append(\"\")\n",
    "\n",
    "    # 4) Return all collected data for downstream TTS\n",
    "    return chunk_paths, audio_emotions_list, text_emotions_list, transcripts_list\n",
    "\n",
    "\n",
    "\"\"\"if __name__ == \"__main__\":\n",
    "    # 0) Record from mic (press 's' to start, 'q' to stop)\n",
    "    try:\n",
    "        # Only record if neutral_input.wav doesn't already exist or you want to re-record\n",
    "        if not os.path.exists(FILENAME):\n",
    "            print(\"No existing recording found. Starting recorder.\")\n",
    "            start_recording()\n",
    "        else:\n",
    "            print(f\"Using existing file {FILENAME} (delete it to re-record).\")\n",
    "\n",
    "        # 1) Process file\n",
    "        process_audio_file(FILENAME)\n",
    "\n",
    "    except Exception as main_e:\n",
    "        print(\"Fatal error:\", main_e)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d08b078f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing recording found. Starting recorder.\n",
      "🎙️ Press 's' to start recording, and 'q' to stop.\n",
      "🔴 Recording started...\n",
      "🛑 Recording stopped.\n",
      "✅ Audio saved to neutral_input.wav (samples: 164320, sr: 16000)\n",
      "📊 Waveform plot saved to input_waveform.png\n",
      "🔪 Split into 2 chunks (silence_thresh=-64.5 dBFS)\n",
      "✅ Saved 2 chunk files to 'chunks/'\n",
      "\n",
      "🎧 Combined Emotion Predictions (Audio + Text):\n",
      "chunk_0.wav ➤ Audio: neutral (0.93) | Text: joy (0.99)\n",
      "📝 Transcript: \"What a beautiful day, everything feels bright for love joy and bursting with energy.\"\n",
      "\n",
      "chunk_1.wav ➤ Audio: neutral (1.00) | Text: surprise (0.57)\n",
      "📝 Transcript: \"I can't stop smiling, life just feels amazing right now.\"\n",
      "\n",
      "✅ Synthesized chunk saved to synthesized\\synth_0.wav\n",
      "✅ Synthesized chunk saved to synthesized\\synth_1.wav\n",
      "🎉 Final combined audio saved to final_output.wav\n",
      "🎉 Pipeline completed successfully!\n",
      "\n",
      "📊 Displaying Final Output Waveform:\n",
      "📊 Waveform plot saved to output_waveform.png\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Coqui TTS setup ----------------\n",
    "from TTS.api import TTS\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Use a voice cloning / multi-speaker model\n",
    "TTS_MODEL_NAME = \"tts_models/multilingual/multi-dataset/your_tts\"  # replace with your chosen TTS model\n",
    "tts = TTS(TTS_MODEL_NAME)\n",
    "\n",
    "SYNTH_DIR = \"synthesized\"\n",
    "FINAL_OUTPUT = \"final_output.wav\"\n",
    "os.makedirs(SYNTH_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- Synthesize one chunk ----------------\n",
    "def synthesize_chunk(text, reference_wav, emotion, output_path):\n",
    "    \"\"\"\n",
    "    text: string to synthesize\n",
    "    reference_wav: path to your neutral input (voice reference for cloning)\n",
    "    emotion: string, e.g., \"happy\", \"sad\" (depends on TTS model support)\n",
    "    output_path: path to save the chunk\n",
    "    \"\"\"\n",
    "    # style/emotion support depends on model\n",
    "    tts.tts_to_file(\n",
    "        text=text,\n",
    "        speaker_wav=reference_wav,\n",
    "        style=emotion,\n",
    "        language=\"en\",  # set language if needed\n",
    "        file_path=output_path\n",
    "    )\n",
    "    print(f\"✅ Synthesized chunk saved to {output_path}\")\n",
    "\n",
    "# ---------------- Combine all chunks ----------------\n",
    "def combine_chunks(synth_dir=SYNTH_DIR, output_path=FINAL_OUTPUT):\n",
    "    combined = None\n",
    "    files = sorted(os.listdir(synth_dir))\n",
    "    for file_name in files:\n",
    "        if not file_name.endswith(\".wav\"):\n",
    "            continue\n",
    "        chunk_audio = AudioSegment.from_file(os.path.join(synth_dir, file_name))\n",
    "        if combined is None:\n",
    "            combined = chunk_audio\n",
    "        else:\n",
    "            combined += chunk_audio\n",
    "    if combined:\n",
    "        combined.export(output_path, format=\"wav\")\n",
    "        print(f\"🎉 Final combined audio saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ No synthesized chunks found to combine.\")\n",
    "\n",
    "# ---------------- Full TTS + Combine Pipeline ----------------\n",
    "def tts_and_combine(chunk_paths, reference_wav=FILENAME, audio_emotions=None, text_emotions=None, transcripts=None):\n",
    "    \"\"\"\n",
    "    chunk_paths: list of chunk wav paths\n",
    "    reference_wav: your neutral recording for voice cloning\n",
    "    audio_emotions: list of predicted audio emotions per chunk\n",
    "    text_emotions: list of predicted text emotions per chunk\n",
    "    transcripts: list of transcribed texts per chunk\n",
    "    \"\"\"\n",
    "    for idx, path in enumerate(chunk_paths):\n",
    "        transcript = transcripts[idx] if transcripts and idx < len(transcripts) else \"\"\n",
    "        if not transcript.strip():\n",
    "            print(f\"{path} ➤ No speech detected, skipping TTS.\")\n",
    "            continue\n",
    "\n",
    "        # Decide which emotion to apply (audio > text or custom logic)\n",
    "        chosen_emotion = audio_emotions[idx] if audio_emotions else \"neutral\"\n",
    "\n",
    "        output_wav = os.path.join(SYNTH_DIR, f\"synth_{idx}.wav\")\n",
    "        synthesize_chunk(transcript, reference_wav, chosen_emotion, output_wav)\n",
    "\n",
    "    # Combine all synthesized chunks\n",
    "    combine_chunks(SYNTH_DIR, FINAL_OUTPUT)\n",
    "\n",
    "# ---------------- Example Usage ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # 0) Record from mic (press 's' to start, 'q' to stop) only if file doesn't exist\n",
    "        if not os.path.exists(FILENAME):\n",
    "            print(\"No existing recording found. Starting recorder.\")\n",
    "            start_recording()\n",
    "        else:\n",
    "            print(f\"Using existing file {FILENAME} (delete it to re-record).\")\n",
    "\n",
    "        # 1) Process audio: split, transcribe, and predict emotions\n",
    "        chunk_paths, audio_emotions, text_emotions, transcripts = process_audio_file(FILENAME)\n",
    "\n",
    "        # 2) Run TTS on each chunk and combine all synthesized audio\n",
    "        tts_and_combine(\n",
    "            chunk_paths,\n",
    "            reference_wav=FILENAME,\n",
    "            audio_emotions=audio_emotions,\n",
    "            text_emotions=text_emotions,\n",
    "            transcripts=transcripts\n",
    "        )\n",
    "\n",
    "        print(\"🎉 Pipeline completed successfully!\")\n",
    "\n",
    "        # --- ADD THIS CODE BLOCK ---\n",
    "        # Check if the final file exists before trying to plot it\n",
    "        if os.path.exists(FINAL_OUTPUT):\n",
    "            print(\"\\n📊 Displaying Final Output Waveform:\")\n",
    "            # Add the save_path argument here\n",
    "            plot_waveform(FINAL_OUTPUT, save_path=\"output_waveform.png\") \n",
    "        # --- END OF ADDED CODE ---\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Fatal error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde5010e-da0e-4201-9e3b-4af24ceeaf51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
